# ğŸŒ LargeKernel-Replication â€” Revisiting Large Kernel Design in CNNs

This repository presents a **faithful PyTorch replication** of the large-kernel
convolutional design proposed in *Scaling Up Your Kernels to 31Ã—31: Revisiting Large Kernel Design in CNNs* 

The goal is **theoretical and architectural fidelity**: translating the paperâ€™s
**kernel decomposition, re-parameterization rules, and block-level structure**
into clean, modular code â€” **without training, datasets, or empirical evaluation** 

The focus is strictly on how **very large effective receptive fields** can be
implemented efficiently using **structural re-parameterization**, rather than
dynamic mechanisms such as attention or gating ğŸ§©.

Paper reference:  [Scaling Up Your Kernels to 31Ã—31: Revisiting Large Kernel Design in CNNs (CVPR 2022)](https://arxiv.org/abs/2203.06717)


---

## Overview â€” Large Kernels Without Large Inference Cost ğŸ”

![Large Kernel Design](images/figmix.jpg)

>Standard CNNs typically rely on stacking small kernels (e.g. 3Ã—3) to grow the
>receptive field gradually. While effective, this approach limits **direct
>long-range spatial interaction** and often increases network depth unnecessarily.

The large-kernel design revisits this assumption by explicitly using
**very large depthwise convolution kernels** (up to 31Ã—31), enabling:

- Direct aggregation of long-range spatial context ğŸŒ
- Shallower yet expressive architectures ğŸ§±
- Strong inductive bias for global feature modeling ğŸ§­

Naively deploying such kernels is computationally expensive.  
The shown design resolves this by introducing **structural re-parameterization**:
a multi-branch formulation during construction that collapses into a
**single equivalent large kernel** at inference time .

---

## Large Kernel Re-Parameterization ğŸ§®

Given an input feature map

$$
X \in \mathbb{R}^{C \times H \times W},
$$

the large-kernel block is expressed as a sum of multiple depthwise convolutions
with different kernel sizes:

$$
Y = X + \sum_{i=1}^{N} X * K_i,
$$

where each

$$
K_i \in \mathbb{R}^{k_i \times k_i}, \quad k_i < k_{\text{max}}.
$$

Each branch captures spatial context at a distinct scale, while all convolutions
remain **depthwise**, preserving computational efficiency ğŸª¶.

---

### Kernel Fusion Rule ğŸ”—

During re-parameterization, all branch kernels are embedded into a single
large kernel of size $k_{\text{max}}$ via zero-padding and summation:

$$
K_{\text{fused}} = \sum_{i=1}^{N} \text{Pad}(K_i, k_{\text{max}}).
$$

This yields a **single depthwise convolution** such that

$$
X * K_{\text{fused}} \equiv X + \sum_{i=1}^{N} X * K_i.
$$

Thus, the **training-time multi-branch structure** and the
**deployment-time single-kernel structure** are mathematically equivalent âœ¨.

---

## Architectural Interpretation ğŸ—ï¸

- Large receptive fields are encoded **structurally**, not dynamically
- No attention, no scale competition, no input-conditioned routing
- Multi-branch design exists only to enable kernel fusion
- Inference uses depthwise convolutions with a large kernel
- Fully compatible with residual connections and pointwise convolutions

The resulting blocks are typically wrapped inside residual structures,
ensuring stable optimization while preserving expressive spatial modeling ğŸ”.

---

## Repository Structure ğŸ—‚ï¸

```bash
LargeKernel-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ visualization.py
â”‚   â”‚
â”‚   â”œâ”€â”€ kernels/
â”‚   â”‚   â””â”€â”€ reparam_rules.py
â”‚   â”‚
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ dwconv.py
â”‚   â”‚   â”œâ”€â”€ lkc_multi_branch.py
â”‚   â”‚   â””â”€â”€ lkc_reparam.py
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â”œâ”€â”€ lkc_block.py
â”‚   â”‚   â””â”€â”€ residual_lkc.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ backbone_stub.py
â”‚   â”‚   â””â”€â”€ lkcnet_stub.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
